{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors and neural nets- a gentle introduction :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5423,  1.3192, -0.9794, -0.9501,  0.6271]])\n",
      "tensor([[ 0.5561, -1.6253, -0.5416,  0.1841,  1.4455],\n",
      "        [ 0.1039,  0.2703,  0.6547,  0.7677,  1.5010],\n",
      "        [ 1.9918, -0.3678,  0.3717,  1.2265, -0.4288],\n",
      "        [ 0.3938,  1.2642,  0.2852, -0.0288, -0.1137]])\n",
      "tensor([[[-1.1147, -0.2965,  0.6244],\n",
      "         [ 1.5739,  1.1801,  1.7625]],\n",
      "\n",
      "        [[-0.4741,  0.0129,  0.2843],\n",
      "         [ 0.6897,  1.4671,  0.2356]],\n",
      "\n",
      "        [[ 0.0891, -1.7094,  0.6834],\n",
      "         [-1.9033, -1.0686,  1.1145]]])\n"
     ]
    }
   ],
   "source": [
    "# Few examples on tensors to understand their structure \n",
    "test1 = torch.randn((1, 5)) # it will give me a one line matrix/ vector, a 1D tensor\n",
    "test2 = torch.randn((4, 5)) # it will give me a matrix, a 2D tensor\n",
    "\n",
    "print(test1)\n",
    "print(test2)\n",
    "\n",
    "test3 = torch.randn((3, 2, 3)) # it will give me a series of tensors of other tensors with dimension 2x3, a 3D tensor\n",
    "print(test3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  6,  9],\n",
      "        [12, 15, 18]])\n",
      "tensor([[5, 7, 9],\n",
      "        [5, 7, 9],\n",
      "        [5, 7, 9]])\n",
      "tensor([[ 6, 15],\n",
      "        [ 6, 15],\n",
      "        [ 6, 15]])\n"
     ]
    }
   ],
   "source": [
    "# Example of inside sum for a tensor according to the dimension chosen, see how it works\n",
    "\n",
    "#test4 is a tensor of other 3 tensors each one having 2x3 dimension\n",
    "\n",
    "test4 = torch.tensor([\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6]\n",
    "     ],\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6]\n",
    "     ],\n",
    "     [\n",
    "       [1, 2, 3],\n",
    "       [4, 5, 6]\n",
    "     ]\n",
    "   ])\n",
    "\n",
    "calapse_tensors = torch.sum(test4, dim=0) #it will sum on the level of the \"slices\"= tensors, it will colapse the slices\n",
    "print(calapse_tensors)\n",
    "\n",
    "calapse_rows = torch.sum(test4, dim=1) \n",
    "# it will sum on the level of all rows per tensor, sum(rows from tensor1) then sum(rows from tensor 2) etx, \n",
    "# imagine the image of colapsiong the rows per each tensor\n",
    "print(calapse_rows)\n",
    "\n",
    "colapse_columns = torch.sum(test4, dim=2)\n",
    "#it will sum on the level of all columns per tensor, sum(columns from tensor 1) etc, iit will colapse the columns per tensor\n",
    "print(colapse_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS: A good source for visualizing the sum within a tensor's elements (depending on the dimension chosen) is [`HERE`](https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Note 1</strong> : Broadcasting conditions: \n",
    "\n",
    "1. Each tensor has at least one dimension.\n",
    "2. When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.2605],\n",
      "          [ 0.6978],\n",
      "          [ 0.4193],\n",
      "          [ 0.3149]],\n",
      "\n",
      "         [[-1.0690],\n",
      "          [-0.2535],\n",
      "          [ 1.6775],\n",
      "          [ 1.0760]]]])\n",
      "tensor([[[-2.1533]],\n",
      "\n",
      "        [[ 0.1137]]])\n",
      "tensor([[[[ 2.7143],\n",
      "          [-1.5025],\n",
      "          [-0.9028],\n",
      "          [-0.6780]],\n",
      "\n",
      "         [[-0.1216],\n",
      "          [-0.0288],\n",
      "          [ 0.1908],\n",
      "          [ 0.1224]]]])\n"
     ]
    }
   ],
   "source": [
    "# Check for broadcasting situation in matmul function\n",
    "\n",
    "# test5 = torch.randn(1, 3, 4, 1)\n",
    "# print(test5)\n",
    "\n",
    "# test6 = torch.randn(3, 1, 1)\n",
    "# print(test6)\n",
    "\n",
    "test5 = torch.randn(1, 2, 4, 1)\n",
    "print(test5)\n",
    "\n",
    "test6 = torch.randn(2, 1, 1)\n",
    "print(test6)\n",
    "\n",
    "torch.matmul(test5, test6).size()\n",
    "print(torch.matmul(test5, test6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming back to the calculation output for one layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Note 2: </strong> \n",
    "I will choose between more ways of having the vector column  ([`source`](https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics)):\n",
    "1. input_tensor.reshape (find put more about it), \n",
    "2. input_tensor.resize_ which is an  in place operation that changes directly the content of a tensor\n",
    "3. or input_tensor.view, which will give a new tensor with the same input tensor data. \n",
    "\n",
    "Here is a great forum thread to [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few options here: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), and [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch._C.Generator object at 0x0000023111BDC1B0>\n",
      "tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])\n",
      "torch.Size([1, 5])\n",
      "tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])\n",
      "tensor([[-0.8948],\n",
      "        [-0.3556],\n",
      "        [ 1.2324],\n",
      "        [ 0.1382],\n",
      "        [-1.6822]])\n"
     ]
    }
   ],
   "source": [
    "### Generate some data\n",
    "data = torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "print(data)\n",
    "\n",
    "# Features are 5 random normal variables\n",
    "features = torch.randn((1, 5)) # a tensor matrix 1x5 dimension, randomly generated according to the N(0,1)\n",
    "print(features)\n",
    "print(features.size())\n",
    "\n",
    "# True weights for our data, random normal variables again\n",
    "weights = torch.randn_like(features)\n",
    "print(weights)\n",
    "\n",
    "# change the shape of one line matrix tensor of weights to one column dimension \n",
    "# for doing later the matrix method of multiplication\n",
    "print(weights.view(5,1))\n",
    "\n",
    "# and a true bias term(a tensor as wel)\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]]) tensor([[0.1595]]) tensor([[0.1595]]) tensor([[0.1595]]) tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "## Calculate the output of this network using the weights and bias tensors  (it works because we are dealing with vectors !!!!)\n",
    "y1 = activation(torch.sum(features * weights) + bias)\n",
    "# OR\n",
    "y2 = activation((features * weights).sum() + bias)\n",
    "\n",
    "# y1 = activation(sum(torch.matmul(features, column_weights), bias))\n",
    "\n",
    "y3= activation(torch.mm(features, weights.view(5,1))+ bias)\n",
    "# OR \n",
    "y33 = activation(torch.matmul(features, weights.view(5,1)) + bias)\n",
    "\n",
    "y333 = activation(sum(torch.matmul(features, weights.view(5,1)), bias))\n",
    "\n",
    "print(y3, y33, y333, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1782, -0.2595, -0.0145, -0.3839],\n",
      "         [-2.9662, -1.0606, -0.3090,  0.9343],\n",
      "         [ 1.5496,  0.5989, -0.6377, -2.2858]],\n",
      "\n",
      "        [[-0.3677, -0.8822,  0.5460,  0.1485],\n",
      "         [-0.7557,  0.3917,  0.7470,  1.3798],\n",
      "         [ 1.2877,  0.8684, -1.3822, -0.9632]]]) tensor([[ 0.1072],\n",
      "        [ 0.6125],\n",
      "        [ 0.3296],\n",
      "        [-0.8763]])\n",
      "tensor([[[ 0.1536],\n",
      "         [-1.8883],\n",
      "         [ 2.3258]],\n",
      "\n",
      "        [[-0.5299],\n",
      "         [-0.8039],\n",
      "         [ 1.0585]]])\n"
     ]
    }
   ],
   "source": [
    "## !!! Calculate the output of a node in the network using matrix multiplication ,  \n",
    "# BUT matmul broatcasts (it allows operations between diferrent sizes of tensor under some conditions, see Note 1 above)\n",
    "tensor1 = torch.randn(2, 3, 4)\n",
    "tensor2 = torch.randn(4,1)\n",
    "print(tensor1, tensor2)\n",
    "torch.matmul(tensor1, tensor2).size()\n",
    "print(torch.matmul(tensor1, tensor2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch.Size([3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on <strong> [Matmul examples](https://pytorch.org/docs/stable/torch.html#torch.matmul)</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When having more than one layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "3\n",
      "torch.Size([3, 2])\n",
      "tensor([[-1.6822],\n",
      "        [ 0.3177]])\n",
      "tensor([[0.1328, 0.1373]])\n"
     ]
    }
   ],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn([1, 3]) # we have a 2D tensor\n",
    "print(features.shape)\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "print(n_input)\n",
    "\n",
    "n_hidden = 2                    # Number of hidden units, we will have 2 nodes in the hidden layer\n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "print(W1.shape)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "print(W2)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "print(B1)\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the output (having one hidden layer with 2 nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note 3 : \n",
    "1. h will be the hidden layer with 2 nodes;\n",
    "\n",
    "2. h is a tensor resulted from the activation function applied to (features * W1 + B1);\n",
    "\n",
    "3. output is a tensor resulted from teh activation function applied to (h * W2 + B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6813, 0.4355]])\n",
      "tensor([[0.3171]])\n"
     ]
    }
   ],
   "source": [
    "h = activation(torch.mm(features, W1) + B1)\n",
    "print(h)\n",
    "Y= activation(torch.mm(h, W2) + B2)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note  4: Shape vs Size of tensors: \n",
    "Have a look [here](https://stackoverflow.com/questions/56856996/difference-in-shape-of-tensor-torch-size-and-torch-size1-in-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5) torch.Size([])\n",
      "tensor([5, 6]) torch.Size([2])\n",
      "2\n",
      "tensor([[5, 6]]) torch.Size([1, 2])\n",
      "1\n",
      "tensor([1, 3]) torch.Size([2])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor(5)\n",
    "print(t1, t1.shape) # tensor(n),  torch.Size([])\n",
    "\n",
    "t11= torch.tensor([5,6]) # a 1D tensor\n",
    "print(t11, t11.shape)\n",
    "print(t11.shape[0])\n",
    "\n",
    "t12= torch.tensor([[5,6]]) #a 2D tensor, it is taken as tensor[1,2]\n",
    "print(t12, t12.shape)\n",
    "print(t12.shape[0])\n",
    "\n",
    "t13= torch.tensor((1,3)) # a 1D tensor\n",
    "print(t13, t11.shape)\n",
    "print(t13.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.tensor([[5, 4, 6, 7],[1, 5, 7, 9]])\n",
    "print(t2, t2.shape) # tensor([n]) torch.Size([1])\n",
    "print(t2.shape[0]) # 1 is for the second dimension meaning 2 tensors as elements\n",
    "print(t2.shape[1]) \n",
    "# shape takes the number of elements along a dimension, 0 is for the first dimension, meaning 4 elements within each tensor\n",
    "# shape[position] works like an array's positions \n",
    "\n",
    "t3 = torch.tensor([[10,2], [5,3], [11,5]])\n",
    "print(t3, t3.shape) # torch.Size([3,2])\n",
    "print(t3.shape[0]) # 3, as number of tensors elements \n",
    "print(t3.shape[1]) # 2 as number of elements within the individual tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[10],\n",
      "         [ 5]],\n",
      "\n",
      "        [[ 4],\n",
      "         [ 5]],\n",
      "\n",
      "        [[ 5],\n",
      "         [ 7]]]) torch.Size([3, 2, 1])\n",
      "3\n",
      "2\n",
      "1\n",
      "tensor([[[[10],\n",
      "          [ 5]],\n",
      "\n",
      "         [[ 4],\n",
      "          [ 5]],\n",
      "\n",
      "         [[ 5],\n",
      "          [ 7]]]]) torch.Size([1, 3, 2, 1])\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "t4 = torch.tensor([[[10],[5]],[[4], [5]], [[5],[7]]]) # a 3D tensor\n",
    "print(t4, t4.shape) #  torch.Size([3, 2, 1])\n",
    "print(t4.shape[0])\n",
    "print(t4.shape[1])\n",
    "print(t4.shape[2])\n",
    "\n",
    "t = torch.unsqueeze(t4, 0) # a 4D tensor, the second argument in unsqueeze add an extra dimension, \n",
    "# in this case 0 means 1 tensor of other 2 tensors of dimension 1x1\n",
    "print(t, t.shape) #  torch.Size([1, 3, 2, 1])\n",
    "print(t.shape[0])\n",
    "print(t.shape[1])\n",
    "print(t.shape[2])\n",
    "print(t.shape[3])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switching between Numpy's arrays and Pytorch's tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78868662, 0.73321195, 0.47463454],\n",
       "       [0.3379168 , 0.85028137, 0.99561204],\n",
       "       [0.54409091, 0.66113114, 0.93360019],\n",
       "       [0.25007664, 0.11012498, 0.82379173]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7887, 0.7332, 0.4746],\n",
       "        [0.3379, 0.8503, 0.9956],\n",
       "        [0.5441, 0.6611, 0.9336],\n",
       "        [0.2501, 0.1101, 0.8238]], dtype=torch.float64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a) \n",
    "b # is the array a transformed into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.57737324, 1.46642389, 0.94926907],\n",
       "       [0.67583361, 1.70056273, 1.99122408],\n",
       "       [1.08818182, 1.32226228, 1.86720038],\n",
       "       [0.50015328, 0.22024995, 1.64758346]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy() # I bring back the tensor b into a numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1547, 2.9328, 1.8985],\n",
       "        [1.3517, 3.4011, 3.9824],\n",
       "        [2.1764, 2.6445, 3.7344],\n",
       "        [1.0003, 0.4405, 3.2952]], dtype=torch.float64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.mul_(2) # an in place operation that changes the orginal tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.15474647, 2.93284778, 1.89853815],\n",
       "       [1.35166722, 3.40112547, 3.98244815],\n",
       "       [2.17636364, 2.64452456, 3.73440075],\n",
       "       [1.00030656, 0.4404999 , 3.29516692]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a  # a has changed as well after the change of \"its\" tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
